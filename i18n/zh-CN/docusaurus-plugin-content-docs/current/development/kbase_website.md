---
sidebar_label: 知识库-网站整站抓取
sidebar_position: 28
---

# 知识库-网站整站抓取

## 功能介绍

网站整站抓取功能允许您从指定的网站根域名开始，按照设定的抓取深度和规则，批量抓取整个网站或网站特定部分的内容到知识库中。该功能特别适用于：

- **官网文档批量导入**：一次性抓取整个产品文档站点
- **知识库迁移**：从旧的文档系统批量迁移到新的知识库
- **竞品分析**：系统性地抓取竞争对手的公开信息
- **内容同步**：定期同步外部知识库或文档站点的最新内容

### 核心特性

- **多层级抓取**：支持1-5层的网站深度抓取，可控制抓取范围
- **智能链接发现**：自动发现和跟踪网站内部链接
- **批量内容处理**：高效处理大量页面，支持并发抓取
- **去重机制**：自动识别和过滤重复内容
- **断点续传**：支持大型抓取任务的中断恢复
- **站点地图支持**：可基于sitemap.xml进行更精准的抓取

<!-- ![kbase_website](/img/kbase/kbase_website.png) -->

## 适用场景

### 1. 文档站点迁移

将整个技术文档网站（如GitBook、Confluence等）的内容批量迁移到知识库系统。

### 2. 产品知识库构建

从产品官网的帮助中心、用户手册等多个页面批量构建完整的产品知识库。

### 3. 行业资讯监控

定期抓取行业门户网站的特定栏目，建立行业动态知识库。

### 4. 内部文档整合

将分散在不同系统中的内部文档统一抓取到中央知识库。

## 操作指南

### 第一步：创建整站抓取任务

1. 进入知识库管理页面，点击"新建知识库"或选择已有知识库
2. 选择"网站整站抓取"作为数据源类型
3. 填写抓取配置信息：
   - **起始URL**：输入网站首页或起始页面地址
   - **抓取深度**：设置抓取层级（1-5层，建议2-3层）
   - **抓取范围**：选择抓取整站或指定子目录
   - **页面过滤**：设置要包含或排除的页面规则
   - **内容过滤**：设置要包含或排除的内容规则
   - **并发设置**：设置同时抓取的页面数量
   - **更新频率**：设置自动更新周期（可选）

<!-- ![kbase_website_create](/img/kbase/kbase_website_create.png) -->

### 第二步：配置抓取策略

#### 深度设置详解

- **1层深度**：仅抓取起始页面
- **2层深度**：抓取起始页面 + 直接链接的页面
- **3层深度**：在2层基础上继续抓取下一级链接
- **4-5层深度**：适用于深层嵌套的大型网站

#### 范围控制

- **整站抓取**：从根域名开始抓取所有可访问页面
- **子目录抓取**：限制在指定的URL路径下
- **模式匹配**：使用通配符或正则表达式定义抓取范围

#### 过滤规则

```text
包含规则示例：
- /docs/*（仅抓取docs目录下的页面）
- *.html（仅抓取HTML页面）
- *help*（抓取URL中包含help的页面）

排除规则示例：
- /admin/*（排除管理后台页面）
- *.pdf（排除PDF文件）
- *login*（排除登录相关页面）
```

### 第三步：执行抓取任务

点击"开始抓取"按钮，系统将自动执行以下步骤：

1. **站点分析**：分析网站结构和可访问的页面链接
2. **任务队列**：根据深度和过滤规则生成抓取任务队列
3. **并发抓取**：按照设定的并发数同时抓取多个页面
4. **内容提取**：智能识别并提取每个页面的主要内容
5. **数据清理**：统一格式化和优化内容结构
6. **去重处理**：识别和合并重复或相似的内容
7. **质量检查**：验证抓取内容的完整性和质量

<!-- ![kbase_website_crawl](/img/kbase/kbase_website_crawl.png) -->

:::tip 抓取建议

- 建议在网络稳定时执行大型抓取任务
- 大型网站建议分批抓取，避免一次性抓取过多内容
- 设置合理的并发数，避免对目标网站造成过大压力
- 定期检查抓取进度，及时处理异常页面

:::

### 第四步：监控抓取进度

在抓取过程中，您可以实时监控：

- **抓取进度**：已完成/总页面数的进度条
- **成功率统计**：成功抓取的页面比例
- **错误日志**：记录无法访问或抓取失败的页面
- **内容预览**：查看已抓取页面的内容质量

### 第五步：内容整理与优化

抓取完成后，进行内容整理：

1. **批量预览**：查看所有抓取到的内容
2. **分类整理**：根据页面来源或内容类型进行分类
3. **质量筛选**：移除质量较差或无关的内容
4. **结构优化**：调整内容层级和组织结构
5. **标签标注**：为内容批量添加标签和分类

<!-- ![kbase_website_preview](/img/kbase/kbase_website_preview.png) -->

### 第六步：测试与调优

1. **问答测试**：使用不同类型的问题测试AI回答效果
2. **覆盖度检查**：确认重要知识点都已包含
3. **准确性验证**：验证答案的准确性和时效性
4. **持续优化**：根据使用反馈持续优化内容

<!-- ![kbase_website_chat](/img/kbase/kbase_website_chat.png) -->

## 最佳实践

### 抓取策略规划

- **深度设置原则**：根据网站结构合理设置抓取深度，避免抓取无关内容
- **范围控制**：明确定义抓取范围，专注于有价值的内容区域
- **并发控制**：根据目标网站的承载能力设置合理的并发数
- **时间规划**：在网站访问量较低的时段执行大型抓取任务

### 内容质量保证

- **预抓取验证**：先小范围测试抓取效果，再执行大规模抓取
- **质量监控**：设置内容质量检查点，及时发现和处理问题
- **人工审核**：对关键内容进行人工审核和校验
- **版本管理**：保留抓取历史，便于回滚和对比

### 性能优化

- **分批处理**：将大型抓取任务分解为多个小批次
- **错误重试**：对失败的页面进行自动重试
- **缓存策略**：利用缓存减少重复抓取
- **资源限制**：设置合理的内存和带宽限制

## 高级配置

### 自定义抓取规则

```yaml
# 抓取配置示例
crawl_config:
  depth: 3
  concurrent: 5
  delay: 1000ms
  include_patterns:
    - "/docs/**"
    - "/help/**"
    - "/faq/**"
  exclude_patterns:
    - "/admin/**"
    - "*.pdf"
    - "*login*"
  content_selectors:
    title: "h1, .title"
    content: ".content, .main, article"
    exclude: ".sidebar, .nav, .footer"
```

### Sitemap集成

支持基于网站sitemap.xml文件进行精准抓取：

- 自动发现和解析sitemap.xml
- 基于sitemap优先级排序
- 支持sitemap索引文件
- 可过滤特定类型的URL

### 增量更新策略

- **时间戳检查**：基于页面最后修改时间判断是否需要更新
- **内容哈希**：通过内容哈希值检测内容变化
- **定期全量**：定期执行全量抓取以确保完整性
- **实时监控**：监控网站更新并触发增量抓取

## 常见问题

### Q: 如何确定合适的抓取深度？

A: 抓取深度选择建议：

- **小型网站（&lt;100页）**：可设置3-4层深度
- **中型网站（100-1000页）**：建议2-3层深度
- **大型网站（&gt;1000页）**：建议1-2层深度，分批抓取
- **文档网站**：通常2-3层可覆盖大部分内容

### Q: 抓取速度太慢怎么办？

A: 速度优化方法：

- 增加并发抓取数量（注意不要过高）
- 缩小抓取范围，排除无关页面
- 检查网络连接和目标网站响应速度
- 考虑在网站访问量低的时段执行

### Q: 如何处理需要登录的页面？

A: 登录页面处理：

- 确认是否有公开访问的版本
- 联系网站管理员获取API访问权限
- 考虑使用其他数据导入方式
- 手动下载后使用文件上传功能

### Q: 抓取任务中断了怎么办？

A: 任务恢复处理：

- 系统支持断点续传，可从中断点继续
- 检查错误日志，排除网络或配置问题
- 必要时可重新启动任务（已抓取内容会保留）
- 考虑调整抓取参数降低失败概率

## 注意事项

:::warning 重要提醒

- 确保您有权抓取目标网站的内容
- 严格遵守网站的robots.txt规则和服务条款
- 合理控制抓取频率，避免对目标网站造成负担
- 大型抓取任务请在业务低峰期执行
- 定期检查抓取内容的准确性和时效性

:::

### 法律与合规

- **版权保护**：确保抓取的内容使用符合版权法规
- **服务条款**：仔细阅读并遵守目标网站的服务条款
- **数据保护**：遵守GDPR等数据保护法规
- **商业使用**：商业用途需获得适当的授权许可

### 技术限制

- **JavaScript渲染**：动态生成的内容可能无法抓取
- **反爬虫机制**：部分网站可能设置反爬虫保护
- **网络限制**：防火墙或网络策略可能影响抓取
- **格式限制**：某些特殊格式的页面可能无法正确解析
